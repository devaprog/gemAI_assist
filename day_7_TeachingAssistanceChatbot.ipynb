{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<b><font color=\"orange\" size=6> Developing Teaching Assistance Chatbot\n",
        " </font></b>\n",
        "\n",
        " **Building a Teaching Assistance Chatbot with Gemini AI**"
      ],
      "metadata": {
        "id": "YKQp97h0dfRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:**\n",
        "\n",
        "This Colab notebook empowers students to create a teaching assistance chatbot that leverages text generation capabilities.\n",
        "\n",
        "The chatbot interacts with Gemini AI through API integration, sending and receiving queries to generate informative responses."
      ],
      "metadata": {
        "id": "mIe-tS6ZlKtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I. Core Concepts:**\n",
        "\n",
        "Before diving into code, let's explore some fundamental concepts:\n",
        "\n",
        "\n",
        "\n",
        "1.   **Text Generation:** This AI technique enables computers to produce human-quality text, like summaries, poems, or creative writing.\n",
        "2.   **API (Application Programming Interface):** APIs act as intermediaries, facilitating communication and data exchange between different software programs. In this case, the API allows our chatbot to interact with Gemini AI.\n",
        "3.   **Gemini AI:** A powerful large language model from Google AI, capable of generating text, translating languages, and answering questions.\n"
      ],
      "metadata": {
        "id": "_O6rx8tkleWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II. Building Blocks (Code Sections):**\n",
        "\n",
        "Now, let's delve into the code step-by-step:"
      ],
      "metadata": {
        "id": "XNmpepUomD0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the dependency using pip:\n",
        "! pip install -U -q google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ANYwYjVJ6Bq",
        "outputId": "1f991bfb-5379-4b6d-bdf6-d8de1ac82c2f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/137.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1)Basic Text - Text Teaching Asisitant:**\n",
        "\n",
        "*Before visiting Code read the below Documentation*\n",
        "\n",
        "Function **get_user_input()** -> <b><font color=\"lightblue\">str</font></b>:\n",
        "\n",
        "    \"\"\"\n",
        "    Prompts the user for their query and returns the input as a string.\n",
        "    \"\"\"\n",
        "\n",
        "Function **provide_response(text_response: str)** -> <b><font color=\"lightblue\">None</font></b>:\n",
        "    \n",
        "    \"\"\"\n",
        "    Displays the formatted response using Markdown.\n",
        "    \"\"\"\n",
        "\n",
        "Function **to_markdown(text: str)** -> <b><font color=\"lightblue\">str</font></b>:\n",
        "    \n",
        "    \"\"\"\n",
        "    Formats text with bullet points and removes unwanted characters.\n",
        "    \"\"\"\n",
        "\n",
        "Function **get_response(model_input: str, stream: bool = False)** -> <b><font color=\"lightblue\">str</font></b>:\n",
        "    \n",
        "    \"\"\"\n",
        "    Sends a query to the Gemini model and returns the generated text response.\n",
        "\n",
        "    Args:\n",
        "        model_input: The user's query to be sent to the model.\n",
        "        stream: (Optional) A boolean flag indicating streaming mode (not used here).\n",
        "\n",
        "    Returns:\n",
        "        The text response generated by the Gemini model.\n",
        "    \"\"\"\n",
        "\n",
        "**Remaining Functions: **\n",
        "\n",
        "**myAssistant Function:**\n",
        "\n",
        "* `myAssistant()`: This function represents the core functionality of the chatbot. Here's a breakdown of its steps:\n",
        "    * Sets the `stream` parameter to `False`.\n",
        "    * Retrieves the user's query using `get_user_input()`.\n",
        "    * Asks the user if they want to limit the response length (`max_limit`). If yes, it appends the user's desired word limit to the query.\n",
        "    * Calls `get_response` to send the user's query to the model and retrieve the generated response.\n",
        "    * Displays the response using `provide_response`.\n",
        "\n",
        "**Main Execution:**\n",
        "\n",
        "* `main()`: This function serves as the entry point for the program. Here's what it does:\n",
        "    * Defines a variable `name` (you can change this).\n",
        "    * Generates a welcome message using the user's name and sends it to the model (stream is set to `False` for a single response).\n",
        "    * Calls `myAssistant()` to initiate the chatbot interaction loop.\n",
        "\n",
        "**Conditional Execution:**\n",
        "\n",
        "* `if __name__ == \"__main__\":`: This line ensures the code within this block only executes when the script is run directly (not imported as a module).\n"
      ],
      "metadata": {
        "id": "5_w1iF4fsjeP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "0Q0HQk6uJTLV",
        "outputId": "1273b0ce-9fdb-4302-9504-0a70b40db27b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Hello User, my name is Gemini. I am an AI Assistant from Google. I am here to help you with any questions you may have. How can I help you today?"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What's your query?: explain about pythagores theorem\n",
            "Do you wish to fix max_limit for output? [Y/N]: y\n",
            "fix max_limit: 20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Pythagoras' theorem states that in a right-angled triangle, the square of the hypotenuse is equal to the sum of the squares of the other two sides."
          },
          "metadata": {}
        }
      ],
      "source": [
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "GOOGLE_API_KEY = \"AIzaSyCrUCzO2suUkeiwLR3NYXG85Erw1X4Zh00\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-pro') # -pro for model => txt query : txt response\n",
        "\n",
        "def get_user_input():\n",
        "    return input(\"What's your query?: \")\n",
        "\n",
        "def provide_response(text_response):\n",
        "    display(to_markdown(text_response))\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "def get_response(model_input, stream):\n",
        "    response = model.generate_content(model_input, stream=stream)\n",
        "    response.resolve() # Waits for response generation\n",
        "    try:\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f'{type(e).__name__}: {e}')\n",
        "        print(response.prompt_feedback) #Notification Feature: if Querry Rejected for any Reason\n",
        "        return \"Sorry, I don't have enough information to answer that right now. Ask me later about this!\"\n",
        "\n",
        "def myAssistant():\n",
        "    stream = False\n",
        "    model_input = get_user_input()\n",
        "\n",
        "    if input(\"Do you wish to fix max_limit for output? [Y/N]: \").lower() == 'y':\n",
        "        model_input = model_input + \" within \" + input('fix max_limit: ') + \" words.\"\n",
        "\n",
        "    provide_response(get_response(model_input, stream))\n",
        "\n",
        "def main():\n",
        "    name = \"User\"  # Change your name here!\n",
        "    provide_response(get_response(f\"hi, Iam {name}\", stream=False))\n",
        "    myAssistant()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2)Incorporating Subject Features:**\n",
        "\n",
        "*Before visiting Code read the below Documentation*\n",
        "\n",
        "- Teaching Assistance Chatbot with Subject Handling\n",
        "\n",
        "- This script creates a text-based teaching assistance chatbot that leverages subject-specific knowledge for improved responses.\n",
        "\n",
        "**Added: a dictionary:**\n",
        "\n",
        "    \"\"\"\n",
        "    Subject-wise Pretext Dictionary (expand and customize as needed)\n",
        "    \"\"\"\n",
        "\n",
        "**Newly Added Function:**\n",
        "\n",
        "Function **get_subject()** -> <b><font color=\"lightblue\">str</font></b>:\n",
        "\n",
        "    \"\"\"\n",
        "    Prompts the user for their subject and returns the input as a string.\n",
        "    \"\"\"\n",
        "\n",
        "**Updated Function:**\n",
        "\n",
        "Function **myAssistant()** -> <b><font color=\"lightblue\">None</font></b>:\n",
        "\n",
        "    \"\"\"\n",
        "    Generates a response to a user query using the subject and user-Querry.\n",
        "\n",
        "    Displays:\n",
        "        A string containing the generated response using provide_response() function.\n",
        "    \"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "adNRVliS6Wb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "GOOGLE_API_KEY = \"AIzaSyCrUCzO2suUkeiwLR3NYXG85Erw1X4Zh00\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-pro') # -pro for model => txt query : txt response\n",
        "\n",
        "def get_user_input():\n",
        "    return input(\"What's your query?: \")\n",
        "\n",
        "def provide_response(text_response):\n",
        "    display(to_markdown(text_response))\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "def get_response(model_input, stream):\n",
        "    response = model.generate_content(model_input, stream=stream)\n",
        "    response.resolve() # Waits for response generation\n",
        "    try:\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f'{type(e).__name__}: {e}')\n",
        "        print(response.prompt_feedback) #Notification Feature: if Querry Rejected for any Reason\n",
        "        return \"Sorry, I don't have enough information to answer that right now. Ask me later about this!\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "New Subject Features are Implemented Below.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Subject-wise Pretext Dictionary (expand and customize as needed)\n",
        "subject_pretexts = {\n",
        "    \"math\": \"In mathematics, \",\n",
        "    \"science\": \"From a scientific perspective, \",\n",
        "    \"history\": \"Throughout history, \",\n",
        "    \"english\": \"In the realm of language arts, \",\n",
        "    \"geography\": \"When exploring the world, \",\n",
        "    \"social studies\": \"Within the social sciences, \",\n",
        "    \"others\": \"\"\n",
        "}\n",
        "\n",
        "# Function to get user's subject choice\n",
        "def get_subject():\n",
        "    print(\"Available subjects include:\")\n",
        "    for subject in subject_pretexts.keys():\n",
        "        print(f\"- {subject}\")\n",
        "    subject_choice = input(\"Choose a subject: \").lower()\n",
        "    if subject_choice not in subject_pretexts.keys():\n",
        "        print(f\"Invalid subject. Please choose from the available options.\")\n",
        "        return get_subject()  # Recursively call to get valid subject\n",
        "    return subject_choice\n",
        "\n",
        "def myAssistant():\n",
        "    stream = False\n",
        "    subject = get_subject()    # subject selection\n",
        "\n",
        "    model_input = get_user_input()\n",
        "\n",
        "    # Prepend subject-specific pretext to the prompt\n",
        "    model_input = f\"{subject_pretexts[subject]}{model_input}\"\n",
        "\n",
        "    if input(\"Do you wish to fix max_limit for output? [Y/N]: \").lower() == 'y':\n",
        "        model_input = model_input + \" within \" + input('fix max_limit: ') + \" words.\"\n",
        "\n",
        "    provide_response(get_response(model_input, stream))\n",
        "\n",
        "def main():\n",
        "    name = \"User\"  # Change your name here!\n",
        "    provide_response(get_response(f\"hi, Iam {name}\", stream=False))\n",
        "    myAssistant()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "Z7cTWz0O6X4A",
        "outputId": "0fca5816-b792-4b01-c6ce-89889434d99d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Hello User, how can I help you today?"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available subjects include:\n",
            "- math\n",
            "- science\n",
            "- history\n",
            "- english\n",
            "- geography\n",
            "- social studies\n",
            "- others\n",
            "Choose a subject: math\n",
            "What's your query?: Explain about bayes theorem\n",
            "Do you wish to fix max_limit for output? [Y/N]: y\n",
            "fix max_limit: 25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Bayes' theorem provides a method for conditional probability calculation, updating the likelihood of an event based on additional information."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><font color=\"LightGreen\">Your Subject Wise Teaching Assistant is READY!</font></b>\n",
        "\n",
        "***Thank You***"
      ],
      "metadata": {
        "id": "RGAQVi71-f8d"
      }
    }
  ]
}